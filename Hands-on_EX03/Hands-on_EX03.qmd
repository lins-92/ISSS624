---
title: "Hands-on Exercise 03"
editor: visual
---

# Geographical Segmentation with Spatially Constrained Clustering Techniques

In geobusiness and spatial policy, it is a common practice to delineate the market or planning area into homogeneous regions by using multivariate data. This exercise is to practice geographical segmentation techniques. Geographical segmentation delineates a homogeneous region by using geographically referenced multivariate data. We will also conduct aspatial hierarchical clustering on the dataset for comparison.

In this hands-on exercise, we are interested to delineate [Shan State](https://en.wikipedia.org/wiki/Shan_State), [Myanmar](https://en.wikipedia.org/wiki/Myanmar) into homogeneous regions by using multiple Information and Communication technology (ICT) measures, namely: Radio, Television, Land line phone, Mobile phone, Computer, and Internet at home.

## 1. Setting up

### Load Packages

There are the packages that will be used:

-   **sf**, **rgdal** and **spdep**: spatial data handling and spatial weights

-   **tidyverse**: manipulation of attribute data and plotting visualisations (aspatial)

-   **tmap**: creating map visualisations

-   **coorplot**, **ggpubr**, and **heatmaply**: multivariate data analysis and visualisation

-   **cluster**, **factoextra**, **dendextend** and **clustGeo**: cluster analysis

```{r}
#| output: false

pacman::p_load(rgdal, spdep, tmap, sf, 
               ggpubr, cluster, factoextra, NbClust,
               heatmaply, corrplot, psych, tidyverse,
               dendextend, GGally)
```

### Import Data

The dataset used in this exercise is Myanmar Township Boundary GIS data and some infocomm data from the **2014 Myanmar Population and Housing Census Myanmar**. For this exercise, we are only using townships from the state of Shan (comprised of Shan (East), Shan (North) and Shan (South)).

The following code chunk imports the geospatial data.

```{r}
shan_sf <- st_read(dsn="data/geospatial",
                   layer="myanmar_township_boundaries") %>%
  filter(ST %in% c("Shan (East)", "Shan (North)", "Shan (South)"))
```

```{r}
glimpse(shan_sf)
```

```{r}
tm_shape(shan_sf) +
  tm_polygons() +
  tm_text("TS", size =0.5)
```

Now for the attribute data:

```{r}
ict <- read_csv ("data/aspatial/Shan-ICT.csv")
```

```{r}
glimpse(ict)
```

The variables Radio, Television, Land line phone, Mobile phone, Computer and Internet at home are the absolute number of households within that township that have access that infocomm technology. For a meaningful comparision between townships, we need to convert these into proportions out of total household in the township. The following code chunk also changes some of the variable names so that the variable names match with the geospatial dataset.

```{r}
ict_derived <- ict %>%
  mutate(`RADIO_PR` = `Radio`/`Total households`*100) %>%
  mutate(`TV_PR` = `Television`/`Total households`*100) %>%
  mutate(`LLPHONE_PR` = `Land line phone`/`Total households`*100) %>%
  mutate(`MPHONE_PR` = `Mobile phone`/`Total households`*100) %>%
  mutate(`COMPUTER_PR` = `Computer`/`Total households`*100) %>%
  mutate(`INTERNET_PR` = `Internet at home`/`Total households`*100) %>%
  rename(`DT_PCODE` =`District Pcode`,`DT`=`District Name`,
         `TS_PCODE`=`Township Pcode`, `TS`=`Township Name`,
         `TT_HOUSEHOLDS`=`Total households`,
         `RADIO`=`Radio`, `TV`=`Television`, 
         `LLPHONE`=`Land line phone`, `MPHONE`=`Mobile phone`,
         `COMPUTER`=`Computer`, `INTERNET`=`Internet at home`) 

```

Join the aspatial data to the geospatial data.

```{r}
shan_sf <- left_join(shan_sf, 
                     ict_derived, 
                     by=c("TS_PCODE"="TS_PCODE"))
```

## 2. Exploratory Data Analysis

We can plot the distribution of technology penetration for each type of infocomm technology. The follow code chunk creates a histogram of the absolute number of households that have adopted each technology type and then we use ggarrange() to combine these plots together.

```{r}
radio <- ggplot(data=ict_derived, 
             aes(x= `RADIO`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue") +
  theme_bw()

tv <- ggplot(data=ict_derived, 
             aes(x= `TV`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")+
  theme_bw()

llphone <- ggplot(data=ict_derived, 
             aes(x= `LLPHONE`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")+
  theme_bw()

mphone <- ggplot(data=ict_derived, 
             aes(x= `MPHONE`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")+
  theme_bw()

computer <- ggplot(data=ict_derived, 
             aes(x= `COMPUTER`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")+
  theme_bw()

internet <- ggplot(data=ict_derived, 
             aes(x= `INTERNET`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")+
  theme_bw()

ggarrange(radio, tv, llphone, mphone, computer, internet, 
          ncol = 3, nrow = 2)
```

We should also visualise the distribution of % technology penetration. Instead of creating multiple plots and combining them, we can also make use of the `facet_wrap()` function of ggplot2 to automatically generate the 6 graphs. First, we need to use `pivot_longer()` to pivot the data from wide to long format such that each observation corresponds to one technology type in a township.

One of the benefits of this method is that it automatically sets the x-axis to the same scale so it is easier to compare between each type of technology.

```{r}
ict_prop <- ict_derived %>%
  pivot_longer(cols= ends_with("_PR"),
               names_to = "tech",
               values_to="PR") %>%
  mutate(tech = str_replace(tech, "_PR", ""))
```

```{r}
ggplot(data = ict_prop,
       aes(x=PR)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue") + 
  facet_wrap(vars(tech)) +
  theme_bw()
```

We can see that computers clearly have the lowest penetration rate with almost all townships having less than 10% of households with this technology. TV is the technology with the highest penetration rate.

We should also plot these spatially. We can plot all the penetration rates at the same time using the `tm_facets()` function.

```{r}
#| fig-width: 10
#| fig-height: 8

tm_shape(shan_sf) +
    tm_polygons(c("RADIO_PR", "TV_PR", "LLPHONE_PR", "MPHONE_PR",
                  "COMPUTER_PR", "INTERNET_PR"),
                style="jenks") +
    tm_facets(sync = TRUE, ncol = 3) +
  tm_legend(legend.position = c("right", "bottom"))+
  tm_layout(outer.margins=0, asp=0)

```

We can see that technology penetration is not equal in the Shan state. Some townships have much higher technology penetration rates for all types of technology. Nonetheless, although a township may have very high penetration of one type of technology, it does not necessarily translate into high penetration rate of other technologies.

## 3. Correlation Analysis

Before conducting cluster analysis, we need to check for multicollinearity. Multicollinearity occurs when independent variables are linearly correlated. it is important to check for multicollinearity because it makes statistical inferences unreliable.

The cor() function computes the correlation coefficients of each pair of variables. We can then use the corrplot.mix() function from the corrplot function to plot a correlogram showing the pairwise correlation values.

```{r}
cluster_vars.cor = cor(ict_derived[,12:17])
corrplot.mixed(cluster_vars.cor,
         lower = "ellipse", 
               upper = "number",
               tl.pos = "lt",
               diag = "l",
               tl.col = "black")
```

We typically consider any correlation coefficient more than 0.85 or less than -0.85 as highly correlated. As such, we should only use one of the COMPUTER_PR or INTERNET_PR for the subsequent analysis.

## 4. Hierarchical Clustering

First, we create a new dataframe with only the variables of interest. Note that we did not include INTERNET_PR because of the multicollinearity issue. Next, we assign the row names using the township names.

```{r}
cluster_vars <- data.frame(ict_derived) %>%
  select("TS", "RADIO_PR", "TV_PR", "LLPHONE_PR", "MPHONE_PR", "COMPUTER_PR")

row.names(cluster_vars) <- cluster_vars$"TS"
head(cluster_vars,10)
```

We no longer need the TS column.

```{r}
shan_ict <- cluster_vars %>%
  select(!1)
```

### Standardisation of Data

Data standardisation is performed such that variables belong to approximately the same range. This is important in hierarchical clustering because it uses distance measures. A variable which has a larger magnitude will have a larger effect on the computed distance than other variables. Standardisation does not change the distribution of the data.

The following chunck applies min-max normalisation. It standardised values to a range of 0-1 with 0 being min value and 1 being the max value.

```{r}
shan_ict.std <- normalize(shan_ict)
summary(shan_ict.std)
```

The following chunk applies z-score normalisation which assumes that the underlying distribution of the data is a normal distribution. Normality should be checked before using this standardisation method.

```{r}
shan_ict.z <- scale(shan_ict)
describe(shan_ict.z)
```

The following code chunk plots the kernel density original and standardised versions of RADIO_PR. The underlying distribution has not changed.

```{r}
r <- ggplot(data=ict_derived, 
             aes(x= RADIO_PR)) +
  geom_density(
                 color="black") +
  ggtitle("Original Data")

s <- ggplot(data=shan_ict.std, 
       aes(x=RADIO_PR)) +
  geom_density(
                 color="black") +
  ggtitle("Min-Max Standardisation")

z <- ggplot(data=as.data.frame(shan_ict.z), 
       aes(x=RADIO_PR)) +
  geom_density(color="black") +
  ggtitle("Z-score Standardisation")

ggarrange(r, s, z,
          ncol = 3,
          nrow = 1)
```

Standardisation is usually used if the variables have very different ranges. In this case, since all the variables are proportions, they are within the range of 0-100 and do not need to be standardised.

### Choosing Clustering Algorithm and Number of Clusters

Hierarchical clustering uses distance measures to measure similarity within clusters and dissimilarity between clusters. As such, we first need to calculate the proximity matrix of distances between each observation. The method tells the algorithm what type of distance to compute.

```{r}
proxmat <- dist(shan_ict, method = 'euclidean')
```

There are many different agglomeration methods when performing hierarchical clustering (ie. how to measure similarity/dissimilarity between clusters). We can use the agnes() function of the cluster package to compute the agglomerative coefficient for each method to find the best method. Values closer to 1 suggest a strong clustering structure.

The following code chunk creates a function to extract the agglomerative coefficient for each method and applies it to the 4 different methods.

```{r}
m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")

ac <- function(x) {
  agnes(shan_ict, method = x)$ac
}

map_dbl(m, ac)

```

We can then use the `hclust()` to conduct the clustering using the specified method and plot the tree.

```{r}
hclust_ward <- hclust(proxmat, method = 'ward.D')
plot(hclust_ward)
```

However, this still does not tell us the optimal number of clusters. There are 3 methods that we can use:

-   [Elbow Method](https://en.wikipedia.org/wiki/Elbow_method_(clustering))

-   [Average Silhouette Method](https://www.sciencedirect.com/science/article/pii/0377042787901257?via%3Dihub)

-   [Gap Statistic Method](https://statweb.stanford.edu/~gwalther/gap)

#### Gap Statistic

The [**gap statistic**](http://www.web.stanford.edu/~hastie/Papers/gap.pdf) compares the total within intra-cluster variation for different values of k with their expected values under null reference distribution (distribution with no obvious clustering) of the data. The estimated optimal clusters (k) will be value that maximises the gap statistic, indicating that there is greatest difference between the clustering of the data and the clustering of a random dataset at that number of k.

clusGap() is used to compute the gap statistic. We can specify the clustering method and maximum number of clusters to consider. If performing k-means clustering, the nstart argument can be used to specify the number of sets of initial centroids. The firstmax method tells the algorithm to report the first local maximum. Because the null reference distribution is formed through Monte Carlo simulations, we need to specify the number of simulations (B=50 here).

```{r}
set.seed(12345)
gap_stat <- clusGap(shan_ict, 
                    FUN = hcut, 
                    K.max = 10, 
                    B = 50)
# Print the result
print(gap_stat, method = "firstmax")
```

Next, we can visualise the plot by using [*fviz_gap_stat()*](https://rpkgs.datanovia.com/factoextra/reference/fviz_nbclust.html) of [**factoextra**](https://rpkgs.datanovia.com/factoextra/) package.

```{r}
fviz_gap_stat(gap_stat)
```

Although the algorithm recommends 1 cluster as the optimum, this is not meaningful. We can instead use the next local maximum (k=6).

#### Elbow Method

The elbow plots the total within sum of squares (measure of the variability of the observations within each cluster) by each clustering over a range of k. At the elbow of the curve, dimishing marginal returns sets in, meaning that each subsequent increase in k results in less and less decrease in variability within each cluster (ie. clusters are getting more cohesive but at a slower rate). To prevent overfitting, we choose the k at the elbow of the curve.

```{r}
fviz_nbclust(shan_ict, hcut, method = "wss")

```

The elbow appears to occur at 2 or 4.

#### Average Silhouette Method

The silhouette value is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The silhouette ranges from âˆ’1 to +1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters. The average silhouette width is the average of the silhouette values of all the clusters. Maximising this value would give the number of clusters with the best intra-cluster cohesion and intra-cluster separation.

The `fviz_nbclust()` function of the **factoextra** package can use the average silhouette method. In the code chunk below, the `find_k()` function of the **dendextend** package uses average silhouette to recommend the optimal number of clusters.

```{r}
num_k <- find_k(hclust_ward)
plot(num_k)
```

### Visualising Clusters

Although all 3 methods have recommended different optimal number of clusters, let's just use k=6 from the gap statistic method.

The follow code chunk creates borders around the clusters so we can see which townships are clustered together.

```{r}
plot(hclust_ward, cex = 0.6)
rect.hclust(hclust_ward, 
            k = 6, 
            border = 2:5)
```

The heatmaply package can be used to build an interactive cluster heatmap to study the distribution of cluster variables at the same time. As the `heatmaply()` function takes a numeric matrix as input, we must convert the shan_ict object first.

```{r}
shan_ict_mat <- data.matrix(shan_ict)

```

The following code chunk creates the interactive heatmap.

```{r}
heatmaply(normalize(shan_ict_mat),
          Colv=NA,
          dist_method = "euclidean",
          hclust_method = "ward.D",
          seriate = "OLO",
          colors = Blues,
          k_row = 6,
          margins = c(NA,200,60,NA),
          fontsize_row = 4,
          fontsize_col = 5,
          main="Geographic Segmentation of Shan State by ICT indicators",
          xlab = "ICT Indicators",
          ylab = "Townships of Shan State"
          )
```

If we are happy with the 6 clusters formed, we can save the cluster assignments to the dataset and map them out. `cutree()` function extracts the cluster assignments

```{r}
groups <- as.factor(cutree(hclust_ward, k=6))

```

The following code chunk combines the cluster assignments to shan_sf dataset.

```{r}
shan_sf_cluster <- cbind(shan_sf, as.matrix(groups)) %>%
  rename(`CLUSTER`=`as.matrix.groups.`)
```

Now we can plot it.

```{r}
qtm(shan_sf_cluster, "CLUSTER")
```

The clusters are fragmented spatially. This is one of the motivations for spatially constrained clustering.

## 5. Spatially Constrained Clustering

First, we need to convert `shan_sf` into SpatialPolygonsDataFrame. This is because the `skater()` function only support **sp** objects such as SpatialPolygonDataFrame.

The code chunk below uses [`as_Spatial()`]{.underline} of **sf** package to convert shan_sf into a SpatialPolygonDataFrame called shan_sp.

```{r}
shan_sp <- as_Spatial(shan_sf)
```

### Defining the Neighbourhood

As always, we must define the neighbourhood (neighbour list). We will be using a queen contiguity matrix.

```{r}
shan.nb <- poly2nb(shan_sp, queen=TRUE)
summary(shan.nb)
```

```{r}

plot(shan_sp, 
     border=grey(.5))
plot(shan.nb, 
     coordinates(shan_sp), 
     col="blue", 
     add=TRUE)
```

### Computing the Minimum Spanning Tree

A spanning tree of an undirected graph (such as the connectivity network above) with V vertices (nodes) refers to a subgraph with V-1 edges such that all vertices are still connected to each other. The minimum spanning tree refers to a spanning tree with the lowest total cost if the edges are weighted.

To compute the minimum spanning tree, we must first weigh the edges with a cost. The code chunk below computes the "cost" of an edge which is pairwise dissimilarity between neighbours' values on the five variables.

```{r}
lcosts <- nbcosts(shan.nb, shan_ict)
```

We can assign the costs to the neighbour list to create the spatial weight matrix.

```{r}
shan.w <- nb2listw(shan.nb,
                   glist=lcosts,
                   style="B")
summary(shan.w)
```

We can now calculate the minimum spanning tree using the `mstree()` function of the **spdep** package. As the minimum spanning tree only has v-1 edges, the dimension of the shan.mst object is only 54, not 55 which is the total number of townships in Shan state.

```{r}
shan.mst <- mstree(shan.w)
dim(shan.mst)
```

We can plot the minimum spanning tree using the code chunk below.

```{r}
plot(shan_sp, border=gray(.5))
plot.mst(shan.mst, 
         coordinates(shan_sp), 
         col="blue", 
         cex.lab=0.7, 
         cex.circles=0.005, 
         add=TRUE)
```

### Computing Spatially Constrained Clusters Using SKATER Method

The code chunk below computes the spatially constrained clusters using the SKATER method. It takes the edges from the minimum spanning tree (not the costs!) and the actual data containing the variable considered. It performs n cuts to get n+1 number of clusters.

```{r}
clust6 <- skater(edges = shan.mst[,1:2], 
                 data = shan_ict, 
                 method = "euclidean", 
                 ncuts = 5)
```

```{r}
clust6
```

We can find the number of townships within each cluster using the following code:

```{r}
table(clust6$groups)
```

Of course, we should also plot the clusters spatially to see the result:

```{r}
#| warning: false

plot(shan_sp, border=gray(.5))
plot(clust6, 
     coordinates(shan_sp), 
     cex.lab=.7,
     groups.colors=c("red", "green" ,"blue", "brown", "pink", "orange"),
     cex.circles=0.005,
     add=TRUE)
```

```{r}
groups_mat <- as.matrix(clust6$groups)

shan_sf_spatialcluster <- cbind(shan_sf_cluster, 
                                as.factor(groups_mat)) %>%
  rename(`SP_CLUSTER`=`as.factor.groups_mat.`)
qtm(shan_sf_spatialcluster, "SP_CLUSTER")
```

We can also use the ggparcoord function from the GGally package to plot a parallel coordinates plot to examine the distribution of the variables in each cluster.

```{r}
ggparcoord(data= shan_sf_spatialcluster,
           columns=c(25:29),
           mapping = aes(color = as.factor(`SP_CLUSTER`)),
           scale="uniminmax",
           alphaLines = 0.5,
           boxplot=TRUE) +
  labs(title = "Parallel Coordinates Plots of Technology Penetration by Spatial Clusters",
       subtitle = "Standardised using Min-Max Normalisation",
       xlab = "% Household Penetration") +
  scale_colour_brewer(name="Cluster",
                      labels = levels(shan_sf_spatialcluster$`SP_CLUSTER`),
                      palette = "Dark2") +
  theme(axis.text.x = element_text(angle = 45, size=8),
        axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        legend.position = "none") +
  facet_wrap(~ SP_CLUSTER)
```
